---
title: "Assignment 3 - Linear models for Cirrhosis dataset"
author: "Sergi Mayol y Toni Garri"
date: "`r Sys.Date()`"
output: html_document
---

# Introduction

During 1974 to 1984, 424 PBC patients referred to the Mayo Clinic qualified for the randomized placebocontrolled trial testing the drug D-penicillamine. Of these, the initial 312 patients took part in the trial and have mostly comprehensive data. The remaining 112 patients didnâ€™t join the clinical trial but agreed to record basic metrics and undergo survival tracking. Six of these patients were soon untraceable after their diagnosis, leaving data for 106 of these individuals in addition to the 312 who were part of the randomized trial.

# Study Problem Statement

The objective of this assignment is to predict the survival state of patients with liver cirrhosis.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Libraries

For this assignment, the following libraries are used:

```{r}
library(GGally)
library(ggplot2)
library(dplyr)
library(gridExtra)
library(ggcorrplot)
library(mlpack) # Used to create the different models used in this assignment
library(caret)  # Used to obtain all the metrics of the different models
```

# Data

This assignment uses the data frame Cirrhosis dataset that can be found in the DATASET folder.

```{r}
dataset <- read.csv("../datasets/cirrhosis.csv")
```

## Data preparation

Let's study the dataset to see which data will be useful to our study. First of all let's transform to factor all the character data to see in the summary all the NA's.

```{r}
temp_dataset <- dataset
temp_dataset[, c("Status", "Drug", "Sex", "Ascites", "Hepatomegaly", "Spiders", "Edema")] <- lapply(temp_dataset[, c("Status", "Drug", "Sex", "Ascites", "Hepatomegaly", "Spiders", "Edema")], as.factor)
summary(temp_dataset)
```

As we can see there are a lot of missing values, and that is a problem that we have to solve in some way.

Another important thing to do previous to deal with the NA's is seeing the type of the data:

```{r}
str(dataset)
```

Let's simplify the name of the dataset to `df` and also to keep the original without modifications.

```{r}
df <- dataset
```

We can remove the ID from the dataset because it's a value that is used only to index the data and has no other important relation with the rest.

```{r}
df <- subset(df, select = -ID)
```

As it is said in the introduction, there are some patients that didn't took neither the drug or the placebo statement, and those values figure as NA's so let's get rid of them changing them to `None`. 

```{r}
df <- df %>% mutate(Drug = ifelse(is.na(Drug), "None", Drug))
```

Once we got rid of the NA's in `Drug` we can plot it to see that like it was said in the introduction, there are 106 people that didn't took anything.

```{r}
ggplot(df, aes(x = Drug)) +
  geom_bar(fill = "skyblue", color = "black", alpha = 0.7) +
  geom_text(stat = "count", aes(label = after_stat(count)), vjust = -0.5, color = "black", size = 4) +
  labs(title = "Drugs taken", x = "Drug", y = "Count")
```

So now let's clean all the data getting rid of the NA's. In this case when it's a chr data we change the NA for `None`, when is a numeric we use the mean of that variable except for Stage that we use the median because it does not make sense to have half a stage.

```{r}
df <- df %>% mutate(Ascites = ifelse(is.na(Ascites), "None", Ascites))
df <- df %>% mutate(Hepatomegaly = ifelse(is.na(Hepatomegaly), "None", Hepatomegaly))
df <- df %>% mutate(Spiders = ifelse(is.na(Spiders), "None", Spiders))
df <- df %>% mutate(Cholesterol = ifelse(is.na(Cholesterol), mean(Cholesterol, na.rm = TRUE), Cholesterol))
df <- df %>% mutate(Alk_Phos = ifelse(is.na(Alk_Phos), mean(Alk_Phos, na.rm = TRUE), Alk_Phos))
df <- df %>% mutate(SGOT = ifelse(is.na(SGOT), mean(SGOT, na.rm = TRUE), SGOT))
df <- df %>% mutate(Copper = ifelse(is.na(Copper), mean(Copper, na.rm = TRUE), Copper))
df <- df %>% mutate(Tryglicerides = ifelse(is.na(Tryglicerides), mean(Tryglicerides, na.rm = TRUE), Tryglicerides))
df <- df %>% mutate(Platelets = ifelse(is.na(Platelets), mean(Platelets, na.rm = TRUE), Platelets))
df <- df %>% mutate(Prothrombin = ifelse(is.na(Prothrombin), mean(Prothrombin, na.rm = TRUE), Prothrombin))
df <- df %>% mutate(Stage = ifelse(is.na(Stage), median(Stage, na.rm = TRUE), Stage))
summary(df)
```

Now that all the data is cleaned we can start to look to other things, for example the `Status` of the patients, and it is main feature that we would use in this study to help us answering to the question raised above.

```{r}
ggplot(df, aes(x = Status)) +
  geom_bar(fill = "skyblue", color = "black", alpha = 0.7) +
  geom_text(stat = "count", aes(label = after_stat(count)), vjust = -0.5, color = "black", size = 4) +
  labs(title = "Status", x = "Status", y = "Count")
```

```{r}
df$AgeInYear <- df$Age / 365.25 # 366
df$AgeInYear
```

```{r}
df$year_group <- cut(df$AgeInYear,
  breaks = seq(0, 100, by = 10),
  include.lowest = TRUE,
  labels = FALSE
)
df$year_group
```
```{r}
ggplot(df, aes(x = year_group)) +
  geom_bar(fill = "skyblue", color = "black", alpha = 0.7) +
  labs(title = "Number of people per decade", x = "Decade group", y = "Count")
```

To see which features will be a good contender to be picked to make the prediction let's make a correlation matrix to see which are more promising.

```{r}
plot_corr <- ggcorr(
  data = df,
  low = "steelblue",
  mid = "white",
  high = "darkred",
  label = TRUE,
  label_alpha = 0,
  angle = -45,
  max_size = 10,
  min_size = 2,
  size = 3,
  hjust = 0.75
)
plot_corr +
  theme(text = element_text(size = 8))
```

As we can see, a combination of all or some of the variables related to medical condition are a good way to try to predict the state of the patient.

```{r}
stage_proportions <- round(prop.table(table(df$Stage)), 2)
pie(stage_proportions, labels = paste(names(stage_proportions), ": ", stage_proportions), col = rainbow(length(stage_proportions)))
legend("topright", legend = names(stage_proportions), fill = rainbow(length(stage_proportions)), cex = 0.8, title = "Stage")
```

This plot shows us that the majority of the patients are in the third stage followed for the fourth stage.

## Data selection

Seeing all the previous information we have decided to split the data and only pick the variables related to the health state of the patient and his stage.
```{r}
stage_df <- subset(df, select = c(Stage, Ascites, Hepatomegaly, Spiders, Cholesterol, Alk_Phos, Bilirubin, Albumin, Copper, SGOT, Tryglicerides, Platelets, Prothrombin))
```

Now we can see the data that will be used to make the predictions.

```{r}
summary(stage_df)
str(stage_df)
```

With another correlation plot we can see better the relation between the variables selected.

```{r}
plot_corr <- ggcorr(
  data = stage_df,
  low = "steelblue",
  mid = "white",
  high = "darkred",
  label = TRUE,
  label_alpha = 0,
  angle = -45,
  max_size = 10,
  min_size = 2,
  size = 3,
  hjust = 0.75
)
plot_corr +
  theme(text = element_text(size = 8))
```

# Predictions

Now that we found the data that we want to use to predict the survival rate of the patient, we can start the predictions.

## Data split

First of all the Status data has to be transformed to be able to use it as labels to see the accuracy of the different models. 

```{r}
labels <- as.numeric(as.factor(df$Status))
labels
```

This function makes a split of the data, returning the train and test data and also the train and test labels given a dataset, the labels, the size of the split and the seed that would be used.

```{r}
train_test_split <- function(x_df, y_labs, test_size, seed) {
  set.seed(seed)
  index <- sample(nrow(x_df), (1 - test_size) * nrow(x_df))

  train_data <- x_df[index, ]
  train_labels <- as.matrix(y_labs[index])

  test_data <- x_df[-index, ]
  test_labels <- as.matrix(y_labs[-index])

  return(list(
    train_data = train_data,
    train_labels = train_labels,
    test_data = test_data,
    test_labels = test_labels
  ))
}
```
 
Function used to create and train the different models.
 
```{r}
run_classifier <- function(classifier, train_data, train_labels, test_data, test_labels) {
  model <- classifier(training = train_data, labels = train_labels)
  model <- model$output_model

  output <- classifier(input_model = model, test = test_data)
  predictions <- output$predictions

  res <- confusionMatrix(
    factor(predictions, levels = c(1, 2, 3)),
    factor(test_labels)
  )

  return(res)
}
```

## Models

Once the data is prepared, it's ready to be used in the models. In this section, each model is given three random subsets of the cleaned dataset to classify as one of the class labels. The models are as follows:

- Perceptron
- Random Forest
- Linear SVM
- Softmax Regression
- Naive Bayes Classifier
- Decision Tree
- Adaptive Boosting

Each run of the model will output the confusion matrix and the evaluation of each run. Later on, it will be studied and commented on in the `Results` section. Additionally, the results of each run will be displayed immediately after every model, as shown in the following sections for each model.

### Perceptron

```{r}
percep_runs <- list(list(1234, stage_df), list(4567, stage_df), list(2805, stage_df))

percep_results <- vector("list", length = length(percep_runs))

for (i in seq_along(percep_runs)) {
  s_ <- percep_runs[[i]][[1]]
  x_ <- percep_runs[[i]][[2]]

  res <- train_test_split(x_df = x_, y_labs = labels, test_size = 0.2, seed = s_)

  model_res <- run_classifier(
    perceptron,
    res$train_data,
    res$train_labels,
    res$test_data,
    res$test_labels
  )

  percep_results[[i]] <- model_res
}

over_percep_results <- sapply(percep_results, function(result) result$overall)

over_percep_df <- as.data.frame(t(over_percep_results))

print(over_percep_df)
```

### Random Forest

```{r}
randft_runs <- list(list(1234, stage_df), list(4567, stage_df), list(2805, stage_df))

randft_results <- vector("list", length = length(randft_runs))

for (i in seq_along(randft_runs)) {
  s_ <- randft_runs[[i]][[1]]
  x_ <- randft_runs[[i]][[2]]

  res <- train_test_split(x_df = x_, y_labs = labels, test_size = 0.2, seed = s_)

  model_res <- run_classifier(
    random_forest,
    res$train_data,
    res$train_labels,
    res$test_data,
    res$test_labels
  )

  randft_results[[i]] <- model_res
}

over_randft_results <- sapply(randft_results, function(result) result$overall)

over_randft_df <- as.data.frame(t(over_randft_results))

print(over_randft_df)
```

### Linear SVM

```{r}
svm_runs <- list(list(1234, stage_df), list(4567, stage_df), list(2805, stage_df))

svm_results <- vector("list", length = length(svm_runs))

for (i in seq_along(svm_runs)) {
  s_ <- svm_runs[[i]][[1]]
  x_ <- svm_runs[[i]][[2]]

  res <- train_test_split(x_df = x_, y_labs = labels, test_size = 0.2, seed = s_)

  model_res <- run_classifier(
    linear_svm,
    res$train_data,
    res$train_labels,
    res$test_data,
    res$test_labels
  )

  svm_results[[i]] <- model_res
}

over_svm_results <- sapply(svm_results, function(result) result$overall)

over_svm_df <- as.data.frame(t(over_svm_results))

print(over_svm_df)
```

### Softmax Regression

```{r}
soft_runs <- list(list(1234, stage_df), list(4567, stage_df), list(2805, stage_df))

soft_results <- vector("list", length = length(soft_runs))

for (i in seq_along(soft_runs)) {
  s_ <- soft_runs[[i]][[1]]
  x_ <- soft_runs[[i]][[2]]

  res <- train_test_split(x_df = x_, y_labs = labels, test_size = 0.2, seed = s_)

  model_res <- run_classifier(
    softmax_regression,
    res$train_data,
    res$train_labels,
    res$test_data,
    res$test_labels
  )

  soft_results[[i]] <- model_res
}


over_soft_results <- sapply(soft_results, function(result) result$overall)

over_soft_df <- as.data.frame(t(over_soft_results))

print(over_soft_df)
```

### Naive Bayes Classifier

```{r}
nbc_runs <- list(list(1234, stage_df), list(4567, stage_df), list(2805, stage_df))

nbc_results <- vector("list", length = length(nbc_runs))

for (i in seq_along(nbc_runs)) {
  s_ <- nbc_runs[[i]][[1]]
  x_ <- nbc_runs[[i]][[2]]

  res <- train_test_split(x_df = x_, y_labs = labels, test_size = 0.2, seed = s_)

  model_res <- run_classifier(
    nbc,
    res$train_data,
    res$train_labels,
    res$test_data,
    res$test_labels
  )

  nbc_results[[i]] <- model_res
}

over_nbc_results <- sapply(nbc_results, function(result) result$overall)

over_nbc_df <- as.data.frame(t(over_nbc_results))

print(over_nbc_df)
```

### Decision Tree

```{r}
dec_tree_runs <- list(list(1234, stage_df), list(4567, stage_df), list(2805, stage_df))

dec_tree_results <- vector("list", length = length(dec_tree_runs))

for (i in seq_along(dec_tree_runs)) {
  s_ <- dec_tree_runs[[i]][[1]]
  x_ <- dec_tree_runs[[i]][[2]]

  res <- train_test_split(x_df = x_, y_labs = labels, test_size = 0.2, seed = s_)

  model_res <- run_classifier(
    decision_tree,
    res$train_data,
    res$train_labels,
    res$test_data,
    res$test_labels
  )

  dec_tree_results[[i]] <- model_res
}

over_tree_results <- sapply(dec_tree_results, function(result) result$overall)

over_tree_df <- as.data.frame(t(over_tree_results))

print(over_tree_df)
```

### Adaptive Boosting

```{r}
adaboost_runs <- list(list(1234, stage_df), list(4567, stage_df), list(2805, stage_df))

adaboost_results <- vector("list", length = length(adaboost_runs))

for (i in seq_along(adaboost_runs)) {
  s_ <- adaboost_runs[[i]][[1]]
  x_ <- adaboost_runs[[i]][[2]]

  res <- train_test_split(x_df = x_, y_labs = labels, test_size = 0.2, seed = s_)

  model_res <- run_classifier(
    adaboost,
    res$train_data,
    res$train_labels,
    res$test_data,
    res$test_labels
  )

  adaboost_results[[i]] <- model_res
}

over_ada_results <- sapply(adaboost_results, function(result) result$overall)

over_ada_df <- as.data.frame(t(over_ada_results))

print(over_ada_df)
```

## Results 

In this section, we will display and discuss the results obtained from the models.

Function to get the best model out of the list of results

```{r}
get_best_model <- function(model_results_list) {
  accuracies <- sapply(model_results_list, function(result) result$overall[1])
  best_model_index <- which.max(accuracies)
  return(model_results_list[[best_model_index]])
}
```


```{r}
confmat_plot <- function(confusion_df, ttl) {
  plot <- ggplot(confusion_df, aes(x = Reference, y = Prediction)) +
    geom_tile(aes(fill = Freq), color = "white") +
    geom_text(aes(label = Freq), vjust = 1) +
    scale_fill_gradient(low = "red", high = "blue") +
    theme_minimal() +
    labs(
      title = ttl,
      x = "Label",
      y = "Prediction"
    )
  plot
}

grid.arrange(
  confmat_plot(data.frame(get_best_model(percep_results)$table), "Perceptron"),
  confmat_plot(data.frame(get_best_model(randft_results)$table), "Random Forest"),
  confmat_plot(data.frame(get_best_model(svm_results)$table), "SVM"),
  confmat_plot(data.frame(get_best_model(soft_results)$table), "Softmax"),
  confmat_plot(data.frame(get_best_model(nbc_results)$table), "Naive Bayes"),
  confmat_plot(data.frame(get_best_model(dec_tree_results)$table), "Decision Tree"),
  confmat_plot(data.frame(get_best_model(adaboost_results)$table), "Adaptive Boost."),
  ncol = 3
)
```

```{r}
models_results <- list(
  percep_results, randft_results, svm_results,
  soft_results, nbc_results, dec_tree_results, adaboost_results
)

accurracies <- vector("list", length = length(models_results))
idx_1 <- 1
for (model_results in models_results) {
  model_accurracy <- vector("list", length = length(model_results))
  idx_2 <- 1
  for (result in model_results) {
    model_accurracy[[idx_2]] <- result$overall[1]
    idx_2 <- idx_2 + 1
  }
  accurracies[[idx_1]] <- model_accurracy
  idx_1 <- idx_1 + 1
}

accuracy_model1 <- unlist(accurracies[[1]])
accuracy_model2 <- unlist(accurracies[[2]])
accuracy_model3 <- unlist(accurracies[[3]])
accuracy_model4 <- unlist(accurracies[[4]])
accuracy_model5 <- unlist(accurracies[[5]])
accuracy_model6 <- unlist(accurracies[[6]])
accuracy_model7 <- unlist(accurracies[[7]])

accuracy_df <- data.frame(
  Model = rep(c("Perceptron", "Random Forest", "SVM", "Softmax", "Naive Bayes", "Decision Tree", "AdaBoost"), each = length(accuracy_model1)),
  Accuracy = c(accuracy_model1, accuracy_model2, accuracy_model3, accuracy_model4, accuracy_model5, accuracy_model6, accuracy_model7)
)

ggplot(accuracy_df, aes(x = Model, y = Accuracy, fill = Model)) +
  geom_boxplot() +
  labs(title = "Grouped Boxplot of Accuracy", y = "Accuracy") +
  theme_minimal()
```
